## Introduction to Distributed Training with Parameter Sharing

### What is Distributed Training?
Distributed Training of Deep Learning models is a training scheme in which the workload to train a Deep Learning(DL) model is divided among multiple processor nodes. These nodes are usually either one or a combination of the following 3 different devices. Graphical Processing Units (GPUs), Neural Processing Units (GPU) and Central Processing Units (CPUs).

With the increase in the model size and dataset size, training the deep learning models in the distributed training setting has become more practical and efficient. There are two main distributed training types:

**Model Parallelism:** Model parallelism is a to-go distributed training method when the DL model to be trained is so large that it does not fit into a single worker in your training bed. Model Parallelism shards different layers/parameters of the model into several workers. Each worker is responsible for a specific portion of the model, and they communicate to update the parameters during training. Hence, forward/backward propagations are carried out sequentially in a round-robin scheme among the workers.

**Data Parallelism:** The data parallelism method shards the data into mutually exclusive portions and sends them to different workers. Notice that each worker sees different dataset samples and only uses its own portion for local batch processing. The workers also periodically communicate with each other to transfer their local knowledge (ex: gradient vector) obtained on their exclusive batch to the other workers. Now, let k be the number of workers in the distributed training environment, and b be the batch size on each worker. With Data Parallelism, the effective batch size while training a DL model becomes b \times k. If we assume negligible communication costs between the workers compared to batch processing time, data parallelism speeds up the training by a factor of ~k.

### More Details on Data Parallelism
In this blog, we are mainly focusing on the Data Parallelism technique and particularly introducing Parameter-Sharing based distributed data parallel methods. As mentioned above, the workers communicate with each other for knowledge transfer. There are two ways of transferring the local knowledge to the other workers: 

**a) Knowledge Transfer with Gradient Sharing:** In this knowledge transfer type, the workers share their gradient vector with the other workers in the distributed training environment. A notable example of this type is  PyTorch’s DistributedDataParallel (DDP) [^1] method as it is widely adopted by researchers in the DL community. In DDP, the workers enter a collective communication phase after each local iteration in which gradient vectors of all the workers are aggregated with a certain policy (ex: averaging), and then the aggregated gradient vector is sent back to all workers for local update. Notice that the information learned from each local batch is exchanged via gradient sharing and model parameters are never shared. Another key observation here is that the initial model parameters and the applied (aggregated) gradient vectors are the same throughout the training for each worker, hence the model parameters on different workers are the same. [^2] 

**b) Knowledge Transfer with Parameter Sharing:** Another way of sharing the local knowledge learned by each worker can be achieved by broadcasting the model parameters in the collective communication phase. The model parameters are then aggregated in a certain policy to form a center variable. For instance, the EASGD [^3] algorithm forms the center variable by taking the average of the worker parameters with respect to space and time whereas the LSGD [^4] algorithm assigns the worker with the smallest loss value as the center variable. After the center variable is established, all the workers are pushed toward the center variable which serves as a consensus point in the parameter space. Note that this additional update is referred to as distributed update and it is applied on top of the local optimization.

#### Advantages of Parameter-Sharing Methods
+ KT with Parameter Sharing enables all workers to individually and independently explore the loss landscape. Consequently, there is a higher chance of landing on minima with better quality. This is not the case in KT with Gradient Sharing as model parameters on different workers are the same.
+ In order to work properly in a stable manner, Gradient-Sharing methods require communication of workers after every local batch iteration which can result in communication overhead. On the other hand, KT with parameter-sharing methods are able to maintain the stability of training when the communication period increases, i.e. the communication occurs less frequently. They can ensure solid training even when the workers communicate after every 4, 8, …, 1024, … processed local batches. This in turn results in lower time spent on inter-worker communication overall.

#### Disdvantages of Parameter-Sharing Methods
+ Parameter-Sharing methods introduce two additional hyperparameters to be tuned which are communication period and pulling force. The communication period determines after how many iterations the workers should start collective communication and apply the distributed update. The pulling force adjusts the strength of the pulling force applied on each worker toward the center variable, i.e. distributed update.


[^1]: “DistributedDataParallel¶.” DistributedDataParallel - PyTorch 2.1 Documentation, pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html. Accessed 28 Jan. 2024. 
[^2]: Model buffers such as batchnorm statistics are broadcasted from the worker with rank 0 to all the remaining workers and this ensures complete consistency on all model parameters.
[^3]: Zhang, Sixin, Anna E. Choromanska, and Yann LeCun. "Deep learning with elastic averaging SGD." Advances in neural information processing systems 28 (2015).
[^4]: Teng, Yunfei, et al. "Leader stochastic gradient descent for distributed training of deep learning models." Advances in Neural Information Processing Systems 32 (2019).
